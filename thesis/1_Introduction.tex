\section{Introduction}\label{sec1}
Model predictions are at the heart of hydrological decision making, they for instance quantify the effect of groundwater abstraction on groundwater levels. An essential step to improve model predictions is calibration of the model parameters. Conventionally, calibration refers to the process of tuning model parameters to optimize the similarity between observed and simulated values, with the objective to find a unique set of optimal parameters, e.g. \cite{refsgaard1997parameterisation}, where the performance of a parameter set is evaluated with a goodness of fit diagnostic such as the Kling-Gupta Efficiency \citep{gupta2009decomposition}. This is a challenging problem to solve, as the parameter space of hydrological models is characterized by many local optima (sometimes hundreds), which are not necessarily close to the global optimum \citep{duan1992effective}. Historically, local calibration methods such as the widely used Levenberg-Marquardt algorithm \citep{beven1992future} have been used for this purpose. Although computationally efficient, these local calibration methods do not attempt to explore all probable parts of parameter space. As a consequence, these local methods often converge to a local optimum, rather than the global optimum \citep{hendrickson1988comparison, blasone2007parameter}. Therefore, it is often recommended to restart optimization from different starting points multiple times, in the hope of arriving at a better optimum. However, if there are many local optima, this strategy is unlikely to succeed, while increasing the computational burden. Global search algorithms have been developed to avoid this problem, at the cost of generally being much slower. A prominent example is the Shuffled Complex Evolution (SCE-UA) method developed at the University of Arizona by \cite{duan1992effective}. 
%Global search methods have been developed to overcome this limitation by probabilistically or systematically exploring the parameter space to locate the global optimum, even in the presence of many local optima. These methods, such as genetic algorithms, simulated annealing, or evolutionary strategies, often rely on stochastic or heuristic techniques and are typically slower but more robust than local methods.

When making decisions based on model predictions, not only the prediction itself is important (global optimum), but also its uncertainty. For some purposes, such as flood forecasting, the uncertainty estimate may even be more important than the prediction itself. Although SCE-UA can reliably find the global optimum, it is inapplicable for parameter uncertainty assessment \citep{vrugt2003shuffled} due to its deterministic rather than probabilistic nature \citep{vrugt2006application}. \cite{vrugt2003shuffled} addressed this by integrating SCE-UA into a probabilistic statistical method called Markov Chain Monte Carlo (MCMC), creating the Shuffled Complex Evolution Metropolis algorithm (SCEM-UA). 

\begin{table*}[hbt!]
\caption{A comparison of the popularity of several MCMC algorithms between all fields of science and hydrology specifically. Popularity is quantified by counting how often the paper introducing the specific algorithms is cited. Three hydrological journals have been selected to indicate the popularity in hydrology: Journal of hydrology, Water Resources Research \& Advances in Water Resources.  These are the three journals where MCMC methods are most discussed, while  specifically tailored to hydrology (\hyperref[appendix search strat]{\textcolor{blue}{Appendix }\ref{appendix search strat}}).}
\label{tab1}
\begin{tabularx}{\textwidth}{lXXl} 
    \toprule
    MCMC Method & citation in all journals & citations in hydrological journals & paper introducing algorithm\\
    \midrule
    %Metropolis & 23718 & 174 & \cite{metropolis1953equation}\\ % remove?
    Metropolis-Hastings & 7922 & 123 & \cite{hastings1970monte}\\
    Differential Evolution (DE) & 565 & 32 & \cite{terbraak2006markov}\\
    DE with snooker updater (DE-SNK) & 338 & 53 & \cite{terbraak2008differential}\\
    DREAM & 777 & 196 & \cite{vrugt2009accelerating}\\
    Multiple-Try DREAM\textsubscript{(zs)} & 360 & 93\phantom{0} & \cite{laloy2012high}\\
    Affine Invariant sampler (AI) & 1903 & 9\phantom{0}\phantom{0} & \cite{goodman2010ensemble} \\
    Hamiltonian Monte Carlo (HMC) & 2154 & 10\phantom{0} & \cite{duane1987hybrid}\\
    No-U-Turn Sampler (NUTS) & 1886 & 17\phantom{0} & \cite{hoffman2014no}\\ 
    \bottomrule
\end{tabularx}
\end{table*}

The advantage of MCMC with regard to parameter uncertainty estimation is that MCMC produces a probability distribution for each parameter, referred to as the posterior. Traditional MCMC algorithms such as Metropolis-Hastings achieve this as follows. Prior knowledge is formalized into a prior probability distribution for each parameter. Additionally, a likelihood function is selected, which quantifies the probability of the observed data given the (sampled) parameter value(s) of the hydrological model. 
%From now on, methods differ depending on the selected MCMC algorithm, here is described how methods similar to the first MCMC algorithm work \citep{metropolis1953equation}. 
With the prior and likelihood defined, sampling can start. 
For each parameter an initial value is determined from which step-wise exploration of parameter space starts. New locations are generated by a proposal distribution and accepted or rejected stochastically, based on an acceptance ratio, which is calculated with the prior and likelihood of the sampled parameter values. These subsequent dependent steps form a Markov chain. 
% Note that more advanced MCMC algorithms exist, that work quite differently. 
Sampling is conventionally split into two phases: during the burn-in phase the chain searches for the most probable areas of the parameter space and during the main sampling phase this area has been reached and is explored. Sampling is generally stopped when some arbitrary threshold of a selected convergence diagnostic is reached. The estimated posterior probability distribution is the distribution formed by by collecting all locations the Markov chain has sampled during the main sampling phase, where the probability equals the relative sampling density. The success of a MCMC algorithm largely depends on whether its chain(s) get stuck in a subspace and on the speed by which they converge to this distribution. % finish of by saying that MCMC become increasingly relevant due to advances in computational power and additionally some algorithms being easily paralizable for use on HPC cluster (MT-DREAM ref, emcee ref). 

Since the inception of MCMC with Random Walk Metropolis \citep{metropolis1953equation}, hundreds of MCMC algorithms have been developed and introduced in scientific literature \citep{brooks2011handbook}. Two of these algorithms are very popular in hydrological literature (\hyperref[tab1]{\textcolor{blue}{Table} \ref*{tab1}}): Metropolis-Hastings and Differential Evolution Adaptive Metropolis (DREAM). Metropolis-Hastings was developed by \cite{hastings1970monte} and has an asymmetric proposal distribution and modified acceptance probability. \cite{hastings1970monte} solved the issue Random Walk Metropolis has with undersampling of parameter values near a parameter bound, which is caused by parameter bounds only being accessible from one side of the bound. The Metropolis-Hastings proposal distribution approaches a symmetric distribution far from parameter bounds and is symmetric in the absence of parameter bounds. It is still very popular due to its simplicity and versatility \citep{robert2004metropolis}. DREAM was developed more recently \citep{vrugt2009accelerating} as a follow-up of SCEM-UA \citep{vrugt2003shuffled}, with an ensemble method called Differential Evolution (DE) as its main building block \citep{terbraak2006markov}. 

Ensemble methods use multiple chains, where each chain is updated using the (current) position of the other chains in the ensemble. \cite{terbraak2008differential} introduced another ensemble algorithm (DE-SNK), which utilises a move called the snooker update in conjunction with the Differential Evolution move, to improve exploration of parameter space. Another powerful ensemble method is the Affine Invariant Ensemble sampler (AI) developed by \cite{goodman2010ensemble}, which is very popular in other disciplines, but almost absent in hydrology (\hyperref[tab1]{\textcolor{blue}{Table} \ref*{tab1}}).

The choice of which MCMC method to use is not a straightforward one to make with the sheer amount of methods available and their respective advantages and disadvantages. Some work has been done on comparing MCMC methods within hydrology. \cite{brunetti2023depth} compared DE, DE-SNK, AI and AI-SNK (AI extended with snooker updater) in several toy problems, a synthetic and one actual case study. They found that ensemble methods are suitable for vadose zone modelling, and recommend (as a guideline) Affine Invariant based samplers below 10 dimensions (i.e. fewer than 10 parameters to be calibrated), Differential Evolution based samplers between 10 and 20 dimensions and discourage both above 20 dimensions. This limited performance of most Ensemble samplers in high dimensions is also recognised by the authors of the Affine Invariant sampler python package emcee \citep{hogg2018data}. Conversely, an Affine Invariant MCMC Ensemble sampler with a particle filter showed good performance in a highly dimensional case study where it was used to infer the real rainfall from both rain and runoff observations \citep{bacci2023comparison}.  Considering Differential Evolution based algorithms: \cite{vrugt2009accelerating} argues that DREAM can efficiently handle problems involving high dimensionality and Multiple-Try DREAM(zs) \citep{laloy2012high} has been specifically created for problems involving high dimensionality, further challenging the poor performance of Ensemble samplers in high dimensions. 

A powerful algorithm called Hamiltonian Monte Carlo \citep{duane1987hybrid}, which is widely considered the most effective approach when dealing with high dimensionality \citep{hogg2018data}, is almost absent from hydrological literature (\hyperref[tab1]{\textcolor{blue}{Table} \ref*{tab1}}). Recently, \cite{bacci2023comparison} compared the performance of Hamiltonian Monte Carlo to the Affine Invariant sampler with a particle filter in the highly dimensional case study described above. \cite{ulzega2023bayesian} argue for the use of Hamiltonian Monte Carlo, while presenting the same case study. 

The No-U-Turn Sampler created by \cite{hoffman2014no} is an extension of Hamiltonian Monte Carlo, providing auto-tuning of the two parameters and preventing the sampler to double back and retrace its steps (hence the name). However, it is rarely used in hydrological literature (\hyperref[tab1]{\textcolor{blue}{Table} \ref*{tab1}}), although it is much more user friendly than Hamiltonian Monte Carlo. In contrast, it is very popular in other fields of science. Additionally, the No-U-Turn Sampler is the main algorithm in the state-of-the-art probabilistic programming framework Stan \citep{carpenter2017stan}. It was used in hydrology by \cite{krapu2022differentiable}, who argue for the use of Hamiltonian Monte Carlo to perform inference for time-varying parameters (which results in a high dimensional problem) in hydrological models, supported by a real world case study, analysing 20 years of streamflow records from the Model Parameter Estimation Experiment \citep{duan2006model}. 

%state knowledge gap 
A rigorous benchmark comparing MCMC algorithms remains absent in hydrology, yet this could aid more robust uncertainty estimation. The benchmarks that do exist either compare a limited number of relevant algorithms, such as \cite{brunetti2023depth} comparing only Ensemble samplers, yet excluding DREAM. And/or they compare algorithms for only a specific case study (which is a specific dimensional problem), leaving the relative performance of the different algorithms as a function of dimensionality undetermined, e.g. \cite{bacci2023comparison}. 

%feasibility
Although it would be interesting to address this scientific gap, by comparing the most popular and powerful MCMC algorithms in hydrology with promising MCMC algorithms from other fields of science in a benchmark of varying dimensionality, this proved unfeasible for a MSc thesis. 
Therefore, the scope was scaled down to comparing an algorithm popular outside hydrology (Goodman and Weare's Affine Invariant sampler), to two algorithms popular in hydrology (Ter Braak's DE and DE-SNK, see \hyperref[tab1]{\textcolor{blue}{Table} \ref*{tab1}}). These three algorithms have the advantage, regarding feasibility, that they can all be implemented with the Python package emcee \citep{foreman2013emcee}. This research is similar to \cite{brunetti2023depth}, but applied to groundwater modelling instead of inverse vadose zone modelling. %These are all Ensemble Samplers, 

The performance of these samplers is evaluated with a calibration exercise, in which each sampler is given an identical calculation budget. The parameters of four steady-state synthetic groundwater flow models (MODFLOW6) are calibrated with a dimensionality ranging from 1 to 5. The importance of prior knowledge for model calibration is investigated by comparing the performance of the samplers using two different priors. Additionally, the number of observations per layer in the groundwater flow models is varied, to gain insight in the optimal number of observations. 

%All three of these algorithms are of similar complexity.
%Where the moves in Ter Braak's algorithms are the main building blocks of DREAM and MT-DREAM respectively, and are thus super relevant. 
In \hyperref[RWM_example]{\textcolor{blue}{Chapter} \ref{RWM_example}} a single step of the Metropolis algorithm is described in detail (selected for its simplicity), adapted from \cite{johnson2022bayes}, to provide the (unfamiliar) reader with some insight in the machinery behind MCMC. In \hyperref[Methods]{\textcolor{blue}{Chapter} \ref{Methods}} the Methodology for comparing the different algorithms is described, followed by the results in \hyperref[results]{\textcolor{blue}{Chapter }\ref{results}}. The results are discussed in \hyperref[Discussion]{\textcolor{blue}{Chapter }\ref{Discussion}}, providing recommendations on which sampler to use at which dimensionality, together with attained insights on how to effectively implement MCMC for hydrogeological model calibration. Finally, \hyperref[Conclusions]{\textcolor{blue}{Chapter }\ref{Conclusions}} wraps everything up with the conclusions. %or hydrogeology? ddd


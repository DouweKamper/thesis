\section{Theoretical Background}\label{RWM_example}
To understand MCMC it is essential to understand Bayes’ rule (\hyperref[eq1]{\textcolor{blue}{Equation} \ref{eq1}}). Bayes’ rule yields the posterior density (probability density distribution of the parameters, $\theta$, given the data), effectively inverting the likelihood, p(data$|\theta$), through multiplication with the prior distribution, p($\theta$), and division by the marginal likelihood, p(data). The marginal likelihood is obtained by integrating the joint density, p(data, $\theta$), across the range of $\theta$. With each additional parameter, another dimension is added to the integral, quickly becoming unfeasible to solve analytically. This is where MCMC comes in. Since the posterior density is proportional to the likelihood multiplied by the prior (\hyperref[eq2]{\textcolor{blue}{Equation} \ref{eq2}}), the shape of the posterior is known, just not its height. Conveniently only the shape of the posterior is required to sample from it. 

\begin{equation}
p(\theta|data) = \frac{p(data|\theta)p(\theta)}{p(data)}\label{eq1}
\end{equation}

\begin{equation}
p(\theta|data) \propto p(data|\theta)p(\theta)\label{eq2}
\end{equation}
\newline %important for formatting for this specific equation (because it is not in the 'align' environment)
\indent Consider a single parameter model with a Gaussian prior and likelihood. The selected prior for parameter $\theta$ has a mean of zero and standard deviation of one. The prior distribution is given by \hyperref[eq3]{\textcolor{blue}{Equation} \ref{eq3}} and its corresponding probability density function (pdf) is given by \hyperref[eq3b]{\textcolor{blue}{Equation} \ref{eq3b}}. In this example the mean defined by parameter $\theta$ is unknown. However, the data is known to vary normally around this unknown mean with a standard deviation of 0.75. Additionally, a single observation of 6.25 has been made. With this information the likelihood and its pdf can be defined by \hyperref[eq4]{\textcolor{blue}{Equation} \ref{eq4}} and \hyperref[eq4b]{\textcolor{blue}{Equation} \ref{eq4b}} respectively.
\begin{align}
    \theta      &\sim N(0, 1^2)             \label{eq3} \\
    p(\theta)      &= \frac{1}{\sqrt{2 \pi}} \exp{ \left[ - \frac{\theta^2}{2} \right]} \label{eq3b} \\
    data|\theta &\sim N(\theta, 0.75^2)  \label{eq4} \\
    p(data|\theta) &= \frac{1}{\sqrt{2 \pi \cdot 0.75^2}} \exp{ \left[ - \frac{(6.25 - \theta)^2}{2 \cdot 0.75^2} \right]} \label{eq4b}
\end{align}
\indent Suppose, the current position of the chain is $\theta_{t} = 3$. In order to determine the next step, a proposal distribution needs to be selected. The Metropolis algorithm requires a symmetric proposal distribution. For the sake of simplicity a uniform proposal distribution is selected, but e.g. a Gaussian proposal distribution is also possible. The selected uniform distribution has a half width of $w = 1$ and is centred around the current position of the chain, $\theta_{t} = 3$, resulting in: $\text{Unif}(2,4)$. 

If all proposals were accepted, this would not result in convergence to regions of posterior density, but in completely random walk behaviour (often called the drunkard's walk). %alternative first sententence: A proposal generated by this distribution is not always accepted as this would not result in convergence to regions of posterior density, but in completely random walk behaviour, also called the drunkards walk. 
If instead only proposals with higher relative posterior density than the current position were accepted, this would increase the risk of the chain getting stuck in a local optimum, due to the limited reach of the proposal distribution. 
%due to the proposal distribution being unable to generate proposals of higher posterior density, as there would be none in the near proximity. 
\cite{metropolis1953equation} found the ideal balance for accepting/rejecting proposals to be determined by \hyperref[eq5]{\textcolor{blue}{Equation} \ref{eq5}}: 
\begin{equation}
    \mathrm{\alpha} = \min \{  1,\; \frac{p(data|\theta_{t+1} )p(\theta_{t+1})}{p(data|\theta_{t})p(\theta_{t})} \} \label{eq5}\\
\end{equation} 

\noindent Where $\alpha$ is the acceptance probability, $\theta_{t}$ is the current position and $\theta_{t+1}$ is the proposal for the next position. Note that both the numerator and denominator of \hyperref[eq5]{\textcolor{blue}{Equation} \ref{eq5}} are proportional to the posterior density at that location (\hyperref[eq2]{\textcolor{blue}{Equation} \ref{eq2}}). The equation dictates that proposals with a higher posterior density relative to the current position are always accepted, while proposals with lower posterior density are only accepted stochastically. 

Suppose, Unif(2,4) randomly generates the proposal $\theta_{t+1}  = 2.933$. The acceptance probability of this proposal can be calculated by filling $\theta_{t}$ and $\theta_{t+1}$ into \hyperref[eq5]{\textcolor{blue}{Equation} \ref{eq5}} (requiring substitution of \hyperref[eq3b]{\textcolor{blue}{Equation} \ref{eq3b}} and \hyperref[eq4b]{\textcolor{blue}{Equation} \ref{eq4b}} into \hyperref[eq5]{\textcolor{blue}{Equation} \ref{eq5}}). The acceptance probability for this proposal is $\alpha \approx (5.4 \cdot 10^{-3} \, \cdot \, 3.0 \cdot 10^{-5})/(4.4 \cdot 10^{-3} \, \cdot \, 4.5 \cdot 10^{-5}) \approx  0.83$. In order to determine whether the proposal is accepted a random draw ($r$) is made from Unif(0,1). Say, the value drawn for $r$ equals 0.72, then the proposal will be accepted (as $\alpha>r$) and the chain moves towards $\theta = 2.933$.

Now that the chain has moved to a new location, the process is repeated. For the next step, a new proposal will be generated from a proposal distribution centred around $\theta = 2.933$ and accepted stochastically by applying \hyperref[eq5]{\textcolor{blue}{Equation} \ref{eq5}}. %explain convergence, when do we stop and why, also possibly mentino burn-in (preferably not, because I don't use the concept burn-in yet).

%explain that a MCMC consists of several of such steps (until convergence or max number iterations is reached?), and maybe shortly explain which aspects differ for different schemes (e.g. the acceptance probabilities, the distributions they can handle etc) 
% Oh, and then you refer to the next chapter where you work that out :) 